{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X.values\n",
    "y = y.astype(int).values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,\n",
       "        18.,  18., 126., 136., 175.,  26., 166., 255., 247., 127.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        30.,  36.,  94., 154., 170., 253., 253., 253., 253., 253., 225.,\n",
       "       172., 253., 242., 195.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253., 253., 253.,\n",
       "       253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        18., 219., 253., 253., 253., 253., 253., 198., 182., 247., 241.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
       "       253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  11., 190., 253.,  70.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240.,\n",
       "       253., 253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 249., 253.,\n",
       "       249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253.,\n",
       "       250., 182.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 114.,\n",
       "       221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  23.,  66., 213., 253., 253., 253., 253., 198.,  81.,\n",
       "         2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n",
       "       253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  55.,\n",
       "       172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135.,\n",
       "       132.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=True)\n",
    "img = X[0].reshape(28, 28)\n",
    "ax.imshow(img, cmap='Greys')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/11_4.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[y == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/11_4.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize to [-1, 1]\n",
    "X = ((X / 255.) - .5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=10000, random_state=123, stratify=y)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_temp, y_temp, test_size=5000, random_state=123, stratify=y_temp)\n",
    "\n",
    "\n",
    "# optional to free up some memory by deleting non-used arrays:\n",
    "del X_temp, y_temp, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -0.9372549 ,\n",
       "       -0.04313725,  0.51372549, -0.58431373, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -0.35686275,  0.99215686,  0.99215686,\n",
       "       -0.27843137, -1.        , -1.        , -0.6       ,  0.28627451,\n",
       "       -0.68627451, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        0.23921569,  0.99215686,  0.99215686, -0.27843137, -1.        ,\n",
       "       -0.84313725,  0.91372549,  0.99215686, -0.06666667, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        ,  0.23921569,  0.99215686,\n",
       "        0.94509804, -0.58431373, -1.        , -0.45882353,  0.99215686,\n",
       "        0.99215686,  0.4745098 , -0.89019608, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -0.92941176,  0.80392157,  0.99215686,  0.89803922, -0.81176471,\n",
       "       -1.        ,  0.08235294,  0.99215686,  0.99215686,  0.99215686,\n",
       "       -0.36470588, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -0.96862745,  0.2627451 ,  0.99215686,\n",
       "        0.99215686,  0.09803922, -1.        , -0.96862745,  0.88235294,\n",
       "        0.99215686,  0.30196078, -0.23137255,  0.39607843, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -0.19215686,  0.99215686,  0.99215686,  0.86666667, -0.5372549 ,\n",
       "       -1.        , -0.10588235,  0.99215686,  0.99215686,  0.05882353,\n",
       "       -0.81960784,  0.36470588, -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -0.73333333,  0.88235294,  0.99215686,\n",
       "        0.99215686, -0.34117647, -1.        , -0.85098039,  0.71764706,\n",
       "        0.99215686,  0.63137255, -0.90588235, -1.        , -0.79607843,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        0.12156863,  0.99215686,  0.99215686,  0.70196078, -0.76470588,\n",
       "       -1.        , -0.56862745,  0.99215686,  0.99215686, -0.25490196,\n",
       "       -1.        , -1.        , -0.52156863, -0.63137255, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -0.96862745,  0.70980392,  0.99215686,\n",
       "        0.91372549, -1.        , -1.        , -0.96862745,  0.12156863,\n",
       "        0.99215686,  0.99215686,  0.52156863,  0.27843137,  0.27843137,\n",
       "        0.45882353,  0.70196078, -0.97647059, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -0.96862745,  0.70196078,  0.99215686,  0.97647059,  0.57647059,\n",
       "        0.57647059,  0.60784314,  0.99215686,  0.99215686,  0.99215686,\n",
       "        0.99215686,  0.99215686,  0.99215686,  1.        ,  0.38823529,\n",
       "       -0.97647059, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -0.27058824,\n",
       "        0.88235294,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "        0.99215686,  0.99215686,  0.90588235,  0.82745098,  0.49803922,\n",
       "       -0.01960784, -0.60784314, -0.97647059, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -0.78039216, -0.28627451,\n",
       "        0.52941176,  0.89019608,  0.99215686,  0.99215686,  0.51372549,\n",
       "       -0.67058824, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        ,  0.5372549 ,\n",
       "        0.99215686,  0.99215686, -0.41960784, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        ,  0.5372549 ,  0.99215686,  0.58431373,\n",
       "       -0.97647059, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -0.73333333,\n",
       "        0.85098039,  0.96078431, -0.19215686, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -0.96078431,  0.40392157,  0.99215686,  0.52156863,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -0.91372549,\n",
       "        0.99215686,  0.99215686,  0.01176471, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -0.91372549,  0.99215686,  0.99215686,\n",
       "        0.01176471, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -0.98431373, -0.17647059,  0.50588235, -0.41176471, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(5000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000,)\n",
      "(5000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_valid.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "def sigmoid(z):                                        \n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "\n",
    "def int_to_onehot(y, num_labels):\n",
    "\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        ary[i, val] = 1\n",
    "\n",
    "    return ary\n",
    "\n",
    "\n",
    "class NeuralNetMLP:\n",
    "\n",
    "    def __init__(self, num_features, num_hidden, num_classes, random_seed=123):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        \n",
    "        # hidden\n",
    "        self.weight_h = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_hidden, num_features))\n",
    "        print(f\"num_features: {num_features}\")\n",
    "        print(f\"num_hidden: {num_hidden}\")\n",
    "        print(f\"shape of hidden weights: {self.weight_h.shape}\")\n",
    "        self.bias_h = np.zeros(num_hidden)\n",
    "        \n",
    "        # output\n",
    "        self.weight_out = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_classes, num_hidden))\n",
    "        print(f\"shape of out weights: {self.weight_out.shape}\")\n",
    "        self.bias_out = np.zeros(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hidden layer\n",
    "        # input dim: [n_examples, n_features] dot [n_hidden, n_features].T\n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        z_h = np.dot(x, self.weight_h.T) + self.bias_h\n",
    "        # z_h.min()\n",
    "        # z_h.max()\n",
    "        a_h = sigmoid(z_h)\n",
    "        # a_h.min()\n",
    "        # a_h.max()\n",
    "\n",
    "        # Output layer\n",
    "        # input dim: [n_examples, n_hidden] dot [n_classes, n_hidden].T\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        z_out = np.dot(a_h, self.weight_out.T) + self.bias_out\n",
    "        a_out = sigmoid(z_out)\n",
    "        return a_h, a_out\n",
    "\n",
    "    def backward(self, x, a_h, a_out, y):  \n",
    "    \n",
    "        #########################\n",
    "        ### Output layer weights\n",
    "        #########################\n",
    "        \n",
    "        # onehot encoding\n",
    "        y_onehot = int_to_onehot(y, self.num_classes)\n",
    "        y_onehot.shape\n",
    "        y_onehot[0]\n",
    "        y_train[0]\n",
    "\n",
    "\n",
    "        # Part 1: dLoss/dOutWeights\n",
    "        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n",
    "        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n",
    "        ## for convenient re-use\n",
    "        \n",
    "        # calculate the gradients of the out_weights\n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        \n",
    "        # Loss function based on MSE\n",
    "        # the derivative of MSE is\n",
    "        # d(MSE)/d(ŷ) = (2/n) * Σ(ŷ - y)\n",
    "        # n = y.shape[0]\n",
    "        # a_out.shape\n",
    "        # a_out[0]\n",
    "        # y_onehot[0]\n",
    "        # a_out[0] - y_onehot[0]\n",
    "        d_loss__d_a_out = 2.0 * (a_out - y_onehot) / n\n",
    "        # d_loss__d_a_out.shape\n",
    "\n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_a_out__d_z_out = a_out * (1. - a_out) # sigmoid derivative\n",
    "\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        delta_out = d_loss__d_a_out * d_a_out__d_z_out # \"delta (rule) placeholder\"\n",
    "\n",
    "        # gradient for output weights\n",
    "        \n",
    "        # [n_examples, n_hidden]\n",
    "        d_z_out__dw_out = a_h\n",
    "        \n",
    "        # input dim: [n_classes, n_examples] dot [n_examples, n_hidden]\n",
    "        # output dim: [n_classes, n_hidden]\n",
    "        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n",
    "        d_loss__db_out = np.sum(delta_out, axis=0)\n",
    "        \n",
    "\n",
    "        #################################        \n",
    "        # Part 2: dLoss/dHiddenWeights\n",
    "        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet * dHiddenNet/dWeight\n",
    "        \n",
    "        # [n_classes, n_hidden]\n",
    "        d_z_out__a_h = self.weight_out\n",
    "        \n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        d_loss__a_h = np.dot(delta_out, d_z_out__a_h)\n",
    "        \n",
    "        # [n_examples, n_hidden]\n",
    "        d_a_h__d_z_h = a_h * (1. - a_h) # sigmoid derivative\n",
    "        \n",
    "        # [n_examples, n_features]\n",
    "        d_z_h__d_w_h = x\n",
    "        \n",
    "        # output dim: [n_hidden, n_features]\n",
    "        d_loss__d_w_h = np.dot((d_loss__a_h * d_a_h__d_z_h).T, d_z_h__d_w_h)\n",
    "        d_loss__d_b_h = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n",
    "\n",
    "        return (d_loss__dw_out, d_loss__db_out, \n",
    "                d_loss__d_w_h, d_loss__d_b_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_onehot(y=np.array([0, 1, 2, 3, 4]), num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features: 784\n",
      "num_hidden: 50\n",
      "shape of hidden weights: (50, 784)\n",
      "shape of out weights: (10, 50)\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetMLP(\n",
    "    num_features=28*28,\n",
    "    num_hidden=50,\n",
    "    num_classes=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 50)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "activations, outputs = model.forward(X_train)\n",
    "print(activations.shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n",
      "(10,)\n",
      "(50, 784)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "grad_w_out, grad_b_out, grad_w_hidden, grad_b_hidden = model.backward(x=X_train, a_h=activations, a_out=outputs, y=y_train)\n",
    "print(grad_w_out.shape)\n",
    "print(grad_b_out.shape)\n",
    "print(grad_w_hidden.shape)\n",
    "print(grad_b_hidden.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_epochs = 50\n",
    "minibatch_size = 100\n",
    "\n",
    "\n",
    "def minibatch_generator(X, y, minibatch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, indices.shape[0] - minibatch_size \n",
    "                           + 1, minibatch_size):\n",
    "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "        \n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "        \n",
    "# iterate over training epochs\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # iterate over minibatches\n",
    "    minibatch_gen = minibatch_generator(\n",
    "        X_train, y_train, minibatch_size)\n",
    "    \n",
    "    for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "        break\n",
    "        \n",
    "    break\n",
    "    \n",
    "print(X_train_mini.shape)\n",
    "print(y_train_mini.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation MSE: 0.3\n",
      "Initial validation accuracy: 9.4%\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(targets, probas, num_labels=10):\n",
    "    onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "    return np.mean((onehot_targets - probas)**2)\n",
    "\n",
    "\n",
    "def accuracy(targets, predicted_labels):\n",
    "    return np.mean(predicted_labels == targets) \n",
    "\n",
    "\n",
    "_, probas = model.forward(X_valid)\n",
    "mse = mse_loss(y_valid, probas)\n",
    "\n",
    "predicted_labels = np.argmax(probas, axis=1)\n",
    "acc = accuracy(y_valid, predicted_labels)\n",
    "\n",
    "print(f'Initial validation MSE: {mse:.1f}')\n",
    "print(f'Initial validation accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):\n",
    "    mse, correct_pred, num_examples = 0., 0, 0\n",
    "    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n",
    "        \n",
    "    for i, (features, targets) in enumerate(minibatch_gen):\n",
    "\n",
    "        _, probas = nnet.forward(features)\n",
    "        predicted_labels = np.argmax(probas, axis=1)\n",
    "        \n",
    "        onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "        loss = np.mean((onehot_targets - probas)**2)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        \n",
    "        num_examples += targets.shape[0]\n",
    "        mse += loss\n",
    "\n",
    "    mse = mse/i\n",
    "    acc = correct_pred/num_examples\n",
    "    return mse, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial valid MSE: 0.3\n",
      "Initial valid accuracy: 9.4%\n"
     ]
    }
   ],
   "source": [
    "mse, acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "print(f'Initial valid MSE: {mse:.1f}')\n",
    "print(f'Initial valid accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, num_epochs,\n",
    "          learning_rate=0.1):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_acc = []\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        # iterate over minibatches\n",
    "        minibatch_gen = minibatch_generator(\n",
    "            X_train, y_train, minibatch_size)\n",
    "\n",
    "        for X_train_mini, y_train_mini in minibatch_gen:\n",
    "            \n",
    "            #### Compute outputs ####\n",
    "            a_h, a_out = model.forward(X_train_mini)\n",
    "\n",
    "            #### Compute gradients ####\n",
    "            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h, d_loss__d_b_h = \\\n",
    "                model.backward(X_train_mini, a_h, a_out, y_train_mini)\n",
    "\n",
    "            #### Update weights ####\n",
    "            model.weight_h -= learning_rate * d_loss__d_w_h\n",
    "            model.bias_h -= learning_rate * d_loss__d_b_h\n",
    "            model.weight_out -= learning_rate * d_loss__d_w_out\n",
    "            model.bias_out -= learning_rate * d_loss__d_b_out\n",
    "        \n",
    "        #### Epoch Logging ####        \n",
    "        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n",
    "        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "        train_acc, valid_acc = train_acc*100, valid_acc*100\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_valid_acc.append(valid_acc)\n",
    "        epoch_loss.append(train_mse)\n",
    "        print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n",
    "              f'| Train MSE: {train_mse:.2f} '\n",
    "              f'| Train Acc: {train_acc:.2f}% '\n",
    "              f'| Valid Acc: {valid_acc:.2f}%')\n",
    "\n",
    "    return epoch_loss, epoch_train_acc, epoch_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123) # for the training set shuffling\n",
    "\n",
    "epoch_loss, epoch_train_acc, epoch_valid_acc = train(\n",
    "    model, X_train, y_train, X_valid, y_valid,\n",
    "    num_epochs=50, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
